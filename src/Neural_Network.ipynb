{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def σ(z):\n",
    "    \"\"\" Sigmoid function\"\"\"\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def σp(z):\n",
    "    \"\"\" Sigmoid first derivative\"\"\"\n",
    "    return σ(z) * (1 - σ(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weights():\n",
    "\n",
    "    def __init__(self, lines, columns) -> None:\n",
    "        \"\"\" Initialize the weights matrix. \"\"\"\n",
    "        self.weights_matrix = np.random.randn(lines, columns)\n",
    "\n",
    "        # Shortcuts\n",
    "        self.W = self.weights_matrix\n",
    "        self.T = self.weights_matrix.T\n",
    "        self.shape = self.weights_matrix.shape\n",
    "\n",
    "    def update_weights(self, learning_rate, weight_gradient) -> None:\n",
    "        \"\"\" Update the weights using gradient descent. \"\"\"\n",
    "        self.weights_matrix -= learning_rate * weight_gradient\n",
    "\n",
    "\n",
    "class Biases():\n",
    "\n",
    "    def __init__(self, lines) -> None:\n",
    "        \"\"\" Initialize the biases vector. \"\"\"\n",
    "        self.bias_vector = np.random.randn(lines, 1)\n",
    "\n",
    "        # Shortcuts\n",
    "        self.b = self.bias_vector\n",
    "        self.shape = self.bias_vector.shape\n",
    "\n",
    "    def update_biases(self, learning_rate, bias_gradient) -> None:\n",
    "        \"\"\" Update the biases using gradient descent. \"\"\"\n",
    "        self.bias_vector -= learning_rate * bias_gradient.sum(axis=1).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "\n",
    "    def __init__(self, layers: list[int]):\n",
    "        \"\"\"\n",
    "        Sets up the network with the number of neurons in each layer defined by ``layers`` and initializes the weights\n",
    "        and biases using a Normal Distribution N(0,1).\n",
    "        \"\"\"\n",
    "        self.n_layers = len(layers)\n",
    "        self.biases = [Biases(lines=bias) for bias in layers[1:]]\n",
    "        self.weights = [Weights(lines=weights, columns=nodes) for weights, nodes in zip(layers[1:], layers[:-1])]\n",
    "\n",
    "        # Store values to backpropagate\n",
    "        self.Zs = None    # Signals\n",
    "        self.As = None    # Activations\n",
    "\n",
    "    def evaluate_test_data(self, test_data: list[tuple]):\n",
    "        \"\"\" Returns the number of test inputs for which the neural network outputs the correct result. \"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def feedforward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Returns the output of the network for the input ``x``. \"\"\"\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            x = σ(np.dot(W.W, x) + b.b)\n",
    "        return x\n",
    "\n",
    "    def stochastic_gradient_descent(self,\n",
    "                                    training_data: list[tuple],\n",
    "                                    epochs: int,\n",
    "                                    mini_batch_size: int,\n",
    "                                    learning_rate: float,\n",
    "                                    test_data: list[tuple] = None):\n",
    "        \"\"\"\n",
    "        Trains the neural network using mini-batch stochastic gradient descent where the ``training_data`` is a list of\n",
    "        tuples ``(x, y)`` representing the training inputs and the labeled outputs.\n",
    "\n",
    "        If ``test_data`` is provided then the network will be evaluated against the test data after each epoch and\n",
    "        partial progress printed out.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k + mini_batch_size] for k in range(0, len(training_data), mini_batch_size)]\n",
    "\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_weights_and_biases(mini_batch, learning_rate)\n",
    "\n",
    "            if test_data:\n",
    "                print(f\"Epoch {epoch}: {self.evaluate_test_data(test_data)} / {len(test_data)}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch} complete\")\n",
    "\n",
    "    def update_weights_and_biases(self, mini_batch: list[tuple], η: float):\n",
    "        \"\"\"\n",
    "        Updates the network's weights and biases by applying gradient descent using backpropagation to a single mini\n",
    "        batch ``mini_batch`` and a learning rate ``η``.\n",
    "        \"\"\"\n",
    "        m = len(mini_batch)\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(W.shape) for W in self.weights]\n",
    "\n",
    "        # Create mini-batch matrices\n",
    "        X = np.column_stack([mini_batch[i][0] for i in range(m)])\n",
    "        Y = np.column_stack([mini_batch[i][1] for i in range(m)])\n",
    "        nabla_b, nabla_w = self.backpropagate(X, Y)\n",
    "\n",
    "        # Gradient Descent Update\n",
    "        for b, nb in zip(self.biases, nabla_b):\n",
    "            b.update_biases(learning_rate=η, bias_gradient=(nb / m))\n",
    "        for W, NW in zip(self.weights, nabla_w):\n",
    "            W.update_weights(learning_rate=η, weight_gradient=(NW / m))\n",
    "\n",
    "    def backpropagate(self, X: np.ndarray, Y: np.ndarray) -> tuple[list, list]:\n",
    "        \"\"\"\n",
    "        Returns a tuple ``(nabla_b, nabla_w)`` representing the gradient for the cost function of the training examples\n",
    "        where ``X`` is a matrix whose columns are the examples of the mini-batch and ``Y`` is a matrix whose columns are\n",
    "        the labels.\n",
    "        \"\"\"\n",
    "        nabla_b = [None for _ in self.biases]\n",
    "        nabla_w = [None for _ in self.weights]\n",
    "        last_layer = self.n_layers - 1\n",
    "        L = -1\n",
    "\n",
    "        # Feed forward\n",
    "        self.feedforward_training_matrix(X)\n",
    "\n",
    "        # Compute δ of the last layer L\n",
    "        Z_L = self.Zs[L]\n",
    "        A_L = self.As[L]\n",
    "        δ = self.cost_derivative(A_L, Y) * σp(Z_L)\n",
    "\n",
    "        # Compute gradient for the cost function C_X\n",
    "        nabla_b[L] = δ\n",
    "        nabla_w[L] = np.dot(δ, self.As[L - 1].T)\n",
    "\n",
    "        # Backpropagate\n",
    "        for l in reversed(range(1, last_layer)):\n",
    "            Z = self.Zs[l - 1]\n",
    "            AT = self.As[l - 1].T\n",
    "            WT = self.weights[l].T\n",
    "\n",
    "            δ = np.dot(WT, δ) * σp(Z)\n",
    "            nabla_b[l - 1] = δ\n",
    "            nabla_w[l - 1] = np.dot(δ, AT)\n",
    "\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def feedforward_training_matrix(self, A: np.ndarray):\n",
    "        \"\"\"\n",
    "        Updates the signals and the activations arrays of the network where ``A`` is a matrix whose columns are the\n",
    "        examples of the mini-batch.\n",
    "        \"\"\"\n",
    "        self.Zs = []\n",
    "        self.As = [A]\n",
    "\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            Z = np.array(np.dot(W.W, A) + b.b)\n",
    "            A = σ(Z)\n",
    "            self.Zs.append(Z)\n",
    "            self.As.append(A)\n",
    "\n",
    "    @staticmethod\n",
    "    def cost_derivative(network_output: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Returns the derivative of the cost function given the ``network_output`` and the correct label ``y``. \"\"\"\n",
    "        return (network_output - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "def load_data():\n",
    "    filename = \"mnist.pkl.gz\"\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    reshape = lambda x, s: np.reshape(x, (s, 1))\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_data   = [(reshape(x, 784), vectorized_result(y)) for x,y in zip(tr_d[0], tr_d[1])]\n",
    "    validation_data = [(reshape(x, 784), y) for x,y in zip(va_d[0], va_d[1])]\n",
    "    test_data       = [(reshape(x, 784), y) for x,y in zip(te_d[0], te_d[1])]\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    layers = [784, 30, 10]\n",
    "    net = Network(layers)\n",
    "\n",
    "    training_data, validation_data, test_data = load_data_wrapper()\n",
    "    epc = 5\n",
    "    mbs = 10\n",
    "    eta = 3.0\n",
    "\n",
    "    net.stochastic_gradient_descent(training_data=training_data,\n",
    "                                    epochs=epc,\n",
    "                                    mini_batch_size=mbs,\n",
    "                                    learning_rate=eta,\n",
    "                                    test_data=test_data)\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
